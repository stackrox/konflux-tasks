apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: post-bigquery-metrics
spec:
  description: |
    Adds a record to ACS BigQuery table about the pipeline's execution status.

    Find the results in Google Cloud Console, BigQuery, StackRox CI project.

    This task should be called at the beginning of the pipeline so that it can insert a record that would later allow
    us identify any pipelines which were started but did not get completed. This also saves the (approximate) start
    timestamp.
    The task should be called again at the end of the pipeline, in `finally` block with provided aggregate tasks status
    so that the record can be updated with the outcome and the (approximate) pipeline completion timestamp.

    Obviously, such an approach with this task has limitations that we can't capture the status of the overall pipeline
    including the `finally` tasks and that it does not provide visibility into the failures before or after the
    pipeline.
    However, it still may be useful for the situation when the majority of the observed failures is in the pipeline
    itself.
  params:
  - name: AGGREGATE_TASKS_STATUS
    default: ''
    description: |
      Status of normal (non-final) tasks of the pipeline.
      When this task is at the beginning of the pipeline, don't pass anything.
      When the task is placed in the pipeline's finally block, pass here `$(tasks.status)`.
      Ref https://tekton.dev/docs/pipelines/pipelines/#using-aggregate-execution-status-of-all-tasks
  volumes:
  - name: gcp-service-account
    secret:
      optional: false
      secretName: konflux-metrics-gcp-service-account
  steps:
  - name: update-bigquery
    # GCP CLIs aren't available in Quay or in the Red Hat Registry. If the availability of Google's registry becomes a
    # problem for our pipelines stability, we can set up image mirroring.
    image: gcr.io/google.com/cloudsdktool/google-cloud-cli:stable@sha256:6440f5710c5effb864b8adc9d3bf0885a58c06a188cf58e4a9d85bd1a1fd94b0
    volumeMounts:
    - name: gcp-service-account
      mountPath: /mnt/gcp-service-account
    env:
    - name: PIPELINE_RUN_NAME
      # E.g. "scanner-v4-db-on-push-477w8"
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['tekton.dev/pipelineRun']
    - name: PIPELINE_RUN_UID
      # E.g. "d9ba34da-d51a-4104-9c40-c9d537d76ef0"
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['tekton.dev/pipelineRunUID']
    - name: ORIGINAL_PIPELINE_RUN_NAME
      # E.g. "scanner-v4-db-on-push"
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['pipelinesascode.tekton.dev/original-prname']
    - name: REPO_URL
      # E.g. "https://github.com/stackrox/stackrox"
      valueFrom:
        fieldRef:
          fieldPath: metadata.annotations['pipelinesascode.tekton.dev/repo-url']
    - name: SOURCE_BRANCH
      # E.g. "konflux/references/master"
      valueFrom:
        fieldRef:
          fieldPath: metadata.annotations['pipelinesascode.tekton.dev/source-branch']
    - name: PULL_REQUEST_NUMBER
      # E.g. "14577"
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['pipelinesascode.tekton.dev/pull-request']
    - name: COMMIT_SHA
      # E.g. "2bb555465e2b822abb4eb13da2167908670ff5c9"
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['pipelinesascode.tekton.dev/sha']
    script: |
      #!/usr/bin/env bash

      set -euo pipefail

      GCP_PROJECT="acs-san-stackroxci"
      TABLE_NAME="${GCP_PROJECT}.ci_metrics.stackrox_jobs"

      gcloud auth activate-service-account --key-file /mnt/gcp-service-account/konflux-metrics-gcp-service-account
      gcloud config set project "${GCP_PROJECT}"

      AGGREGATE_TASKS_STATUS="$(params.AGGREGATE_TASKS_STATUS)"

      if [[ -z "${AGGREGATE_TASKS_STATUS}" ]]; then
        started_at="CURRENT_TIMESTAMP()"
        stopped_at="NULL"
      else
        started_at="NULL"
        stopped_at="CURRENT_TIMESTAMP()"
      fi

      # Even though PIPELINE_RUN_NAME looks unique, it's not guaranteed to be so. There's a chance the same entropy
      # suffix will be generated over a long period of time. Since we don't want metrics to clash, we append UUID.
      ID="${PIPELINE_RUN_NAME}.${PIPELINE_RUN_UID}"

      successes=0
      errors=0

      while (( errors < 3 )); do
        if (( errors > 0 )); then
          # Sleep for ~3 minutes. One minute is enough for the task to complete in normal conditions and so hopefully
          # this sleep should be sufficient for error conditions like full requests queue to resolve.
          (( delay=120 + RANDOM%120 ))
          echo "Making ${delay} seconds pause before attempting a query"
          sleep "${delay}"
        fi

        if bq query \
            --headless \
            --use_legacy_sql=false \
            --parameter="id::${ID}" \
            --parameter="name::${ORIGINAL_PIPELINE_RUN_NAME}" \
            --parameter="repo::${REPO_URL}" \
            --parameter="branch::${SOURCE_BRANCH}" \
            --parameter="pr_number:INTEGER:${PULL_REQUEST_NUMBER:-NULL}" \
            --parameter="commit_sha::${COMMIT_SHA}" \
            --parameter="outcome::${AGGREGATE_TASKS_STATUS:-NULL}" \
            "
            INSERT INTO ${TABLE_NAME}
              (id, name, repo, branch, pr_number, commit_sha, started_at, stopped_at, outcome, ci_system)
              VALUES
              (@id, @name, @repo, @branch, @pr_number, @commit_sha, ${started_at}, ${stopped_at}, @outcome, 'konflux')
            "
        then
          (( successes+=1 ))
          break
        fi

        (( errors+=1 ))
      done

      if (( successes==0 )); then
        echo >&2 "Could not put info in BigQuery."
        exit 6
      fi
